{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict, Counter\n",
    "import tkinter as tk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "query_expansion_map = {\n",
    "    \"dbms\": \"database management system\",\n",
    "    \"ai\": \"artificial intelligence\",\n",
    "    \"ml\": \"machine learning\",\n",
    "    \"nlp\": \"natural language processing\",\n",
    "    \"introductory\": \"introduction fundamentals basics overview\",\n",
    "    \"lab\": \"laboratory\",\n",
    "    \"easy\": \"foundation beginner fundamentals\" \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_course(course_code):\n",
    "    code_num = int(re.findall(r'\\d+', course_code)[0])\n",
    "\n",
    "    if 5000 <= code_num <= 5009:\n",
    "        return \"Master's Align\"\n",
    "    elif code_num < 5000:\n",
    "        return \"Undergraduate\"\n",
    "    return \"Unclassified\"\n",
    "\n",
    "def is_easy_course(description):\n",
    "    easy_keywords = [\"foundation\", \"beginner\", \"fundamentals\", \"introductory\", \"basic\"]\n",
    "    description = description.lower()\n",
    "    return any(keyword in description for keyword in easy_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_relevance_map = {\n",
    "    \"which courses teach about artificial intelligence?\": [\"CS 4100\", \"CS 5100\", \"CS 5150\", \"CS 5170\", \"CS 7170\", \"CS 7180\", \"CS 4150\"],\n",
    "    \"courses related to machine learning.\": [\"CS 6140\", \"CS 4170\", \"CS 5150\", \"CS 5100\"],\n",
    "    \"show all introductory courses.\": [\"CS 1100\", \"CS 1101\", \"CS 2500\", \"CS 2510\"],\n",
    "    \"which courses cover data structures?\": [\"CS 2500\", \"CS 2510\", \"CS 5010\"],\n",
    "    \"what are some easy courses in ai?\": [\"CS 4050\", \"CS 5150\"],\n",
    "    \"which courses are labs or include lab work?\": [\"CS 1101\", \"CS 4700\"],\n",
    "    \"find a course that teaches databases or dbms.\": [\"CS 3200\", \"CS 5200\"],\n",
    "    \"which course introduces programming in python?\": [\"CS 1100\"],\n",
    "    \"courses that include natural language processing.\": [\"CS 6120\", \"CS 7120\"],\n",
    "    \"what courses are advanced in ai and have high credit hours?\": [\"CS 5100\", \"CS 5170\", \"CS 7170\"],\n",
    "    \"ai and advanced\": [\"CS 5100\", \"CS 5170\", \"CS 7170\"],\n",
    "    \"machine and not learning\": [\"CS 4100\"],\n",
    "    \"database or data or dbms\": [\"CS 3200\", \"CS 5200\", \"CS 6200\"],\n",
    "    \"introductory and (ai or machine learning)\": [\"CS 1100\", \"CS 5100\"],\n",
    "    \"programming and not (web or mobile)\": [\"CS 2500\", \"CS 5010\"],\n",
    "    \"which undergraduate courses require cs 2500 as a prerequisite?\": [\"CS 2510\", \"CS 3200\", \"CS 3500\", \"CS 3700\"],\n",
    "    \"show master's courses that have more than 3 credit hours.\": [\"CS 5100\", \"CS 6140\", \"CS 6200\"],\n",
    "    \"which courses have no prerequisites?\": [\"CS 1100\", \"CS 1990\"],\n",
    "    \"courses that list cs 1101 as a corequisite.\": [\"CS 1100\"],\n",
    "    \"which 4-credit-hour courses are ai-related?\": [\"CS 4100\", \"CS 5100\", \"CS 5150\", \"CS 5170\"],\n",
    "    \"master's align courses only.\": [\"CS 5001\", \"CS 5002\", \"CS 5010\"],\n",
    "    \"courses with lab component and high hours.\": [\"CS 4700\"],\n",
    "    \"undergrad courses with prerequisites.\": [\"CS 2500\", \"CS 2510\", \"CS 3200\", \"CS 3500\"],\n",
    "    \"introductory database courses.\": [\"CS 3200\"],\n",
    "    \"courses that are electives and ai-related.\": [\"CS 4050\", \"CS 4170\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "    return [stemmer.stem(t) for t in tokens if t not in stop_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def build_prerequisite_graph(xml_path):\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    for course in root.findall(\"course\"):\n",
    "        code = course.findtext(\"code\", default=\"\").strip()\n",
    "        if not code or code.lower() == \"none\":\n",
    "            continue\n",
    "        G.add_node(code)\n",
    "\n",
    "        prereq_tag = course.find(\"prerequisites\")\n",
    "        if prereq_tag is not None:\n",
    "            for prereq in prereq_tag.findall(\"prerequisite\"):\n",
    "                prereq_code = prereq.text.strip()\n",
    "                if prereq_code and prereq_code.lower() != \"none\":\n",
    "                    G.add_edge(prereq_code, code)\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    inv_index = defaultdict(set)\n",
    "    documents = {}\n",
    "    meta = {}\n",
    "\n",
    "    for course in root.findall(\"course\"):\n",
    "        code = course.find(\"code\").text.strip()\n",
    "        name = course.find(\"name\").text.strip()\n",
    "        desc = course.find(\"description\").text.strip()\n",
    "        hours = course.find(\"hours\").text.strip()\n",
    "\n",
    "        pre = course.find(\"prerequisites\")\n",
    "        co = course.find(\"corequisites\")\n",
    "\n",
    "        prereqs = [p.text.strip() for p in pre.findall(\"prerequisite\")] if pre is not None else []\n",
    "        coreqs = [c.text.strip() for c in co.findall(\"corequisite\")] if co is not None else []\n",
    "\n",
    "        tokens = preprocess_text(f\"{name} {desc}\")  # Includes unigrams and bigrams\n",
    "        documents[code] = tokens\n",
    "        meta[code] = {\n",
    "            \"name\": name,\n",
    "            \"description\": desc,\n",
    "            \"hours\": int(hours) if hours.isdigit() else 0,\n",
    "            \"prerequisites\": prereqs,\n",
    "            \"corequisites\": coreqs\n",
    "        }\n",
    "\n",
    "        for token in tokens:\n",
    "            inv_index[token].add(code)  # Index both unigrams and bigrams\n",
    "\n",
    "    return inv_index, documents, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_prereq_graph(meta):\n",
    "    graph = defaultdict(set)\n",
    "    for course, data in meta.items():\n",
    "        for prereq in data[\"prerequisites\"]:\n",
    "            if prereq in meta:\n",
    "                graph[prereq].add(course)\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def compute_pagerank(graph, alpha=0.85, max_iter=100, tol=1e-6):\n",
    "    return nx.pagerank(graph, alpha=alpha, max_iter=max_iter, tol=tol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_with_pagerank(results, pagerank, weight=0.2):\n",
    "    return sorted(results, key=lambda x: pagerank.get(x[0], 0) * weight, reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prerequisites(course_code, graph):\n",
    "    return list(graph.predecessors(course_code))\n",
    "\n",
    "def get_all_prerequisites(course_code, graph):\n",
    "    return list(nx.ancestors(graph, course_code))\n",
    "\n",
    "def get_dependent_courses(course_code, graph):\n",
    "    return list(graph.successors(course_code))\n",
    "\n",
    "def get_all_dependents(course_code, graph):\n",
    "    return list(nx.descendants(graph, course_code))\n",
    "\n",
    "def get_recursive_prerequisites(course_code, graph):\n",
    "    return list(nx.ancestors(graph, course_code))\n",
    "\n",
    "def get_recursive_dependents(course_code, graph):\n",
    "    return list(nx.descendants(graph, course_code))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams  # Add to imports at the top\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = re.findall(r\"\\b\\w+\\b\", text.lower())\n",
    "    tokens = [stemmer.stem(t) for t in tokens if t not in stop_words]\n",
    "    # Generate bigrams from stemmed tokens\n",
    "    bigrams = ['_'.join(bigram) for bigram in ngrams(tokens, 2)]  # e.g., \"program_design\"\n",
    "    return tokens + bigrams\n",
    "\n",
    "def preprocess_query(query, expand=False):\n",
    "    tokens = re.findall(r\"\\b\\w+\\b\", query.lower())\n",
    "    if expand:\n",
    "        expanded = []\n",
    "        for t in tokens:\n",
    "            if t in query_expansion_map:\n",
    "                expanded += query_expansion_map[t].split()\n",
    "            else:\n",
    "                expanded.append(t)\n",
    "        tokens = expanded\n",
    "    tokens = [stemmer.stem(t) for t in tokens if t not in stop_words]\n",
    "    # Generate bigrams from stemmed tokens\n",
    "    bigrams = ['_'.join(bigram) for bigram in ngrams(tokens, 2)]\n",
    "    return tokens + bigrams\n",
    "\n",
    "def process_boolean_query(tokens, inv_index):\n",
    "    result = set()\n",
    "    op = \"OR\"\n",
    "    for token in tokens:\n",
    "        if token.upper() in {\"AND\", \"OR\", \"NOT\"}:\n",
    "            op = token.upper()\n",
    "        else:\n",
    "            docs = inv_index.get(token, set())\n",
    "            if op == \"OR\":\n",
    "                result |= docs\n",
    "            elif op == \"AND\":\n",
    "                result &= docs if result else docs\n",
    "            elif op == \"NOT\":\n",
    "                result -= docs\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1, b = 1.5, 0.75\n",
    "\n",
    "def compute_bm25_scores(query_tokens, documents, meta):\n",
    "    N = len(documents)\n",
    "    avg_dl = sum(len(doc) for doc in documents.values()) / N\n",
    "    df = defaultdict(int)\n",
    "\n",
    "    for doc in documents.values():\n",
    "        for term in set(doc):\n",
    "            df[term] += 1\n",
    "\n",
    "    idf = {term: math.log((N - df[term] + 0.5) / (df[term] + 0.5) + 1) for term in query_tokens}\n",
    "    scores = {}\n",
    "\n",
    "    for doc_id, doc in documents.items():\n",
    "        tf = Counter(doc)\n",
    "        dl = len(doc)\n",
    "        score = 0\n",
    "        for term in query_tokens:\n",
    "            if term in tf:\n",
    "                f = tf[term]\n",
    "                weight = 1.5 if '_' in term else 1.0  \n",
    "                score += weight * idf[term] * ((f * (k1 + 1)) / (f + k1 * (1 - b + b * dl / avg_dl)))\n",
    "        if score > 0:\n",
    "            hours = meta.get(doc_id, {}).get(\"hours\", 0)\n",
    "            adjusted_score = score - 0.05 * hours  \n",
    "            scores[doc_id] = adjusted_score\n",
    "\n",
    "    return dict(sorted(scores.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compute_tfidf_scores(query_tokens, documents):\n",
    "    doc_ids = list(documents.keys())\n",
    "    corpus = [' '.join(documents[doc_id]) for doc_id in doc_ids]\n",
    "    query = ' '.join(query_tokens)  # Includes bigrams\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 2))  # Add bigrams\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus + [query])\n",
    "    query_vec = tfidf_matrix[-1]\n",
    "    doc_vecs = tfidf_matrix[:-1]\n",
    "    similarities = cosine_similarity(query_vec, doc_vecs).flatten()\n",
    "    scores = {doc_ids[i]: float(similarities[i]) for i in range(len(doc_ids)) if similarities[i] > 0}\n",
    "    return dict(sorted(scores.items(), key=lambda x: x[1], reverse=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, inv_index, documents, meta, use_bm25=True, use_tfidf=False, expand=True, pagerank=None, boost_weight=0.2):\n",
    "    tokens = preprocess_query(query, expand=expand)\n",
    "    query_lower = query.lower().strip()\n",
    "\n",
    "    # Determine course level filters\n",
    "    is_master = \"master's\" in query_lower and \"align\" not in query_lower\n",
    "    is_align = \"master's align\" in query_lower or \"align\" in query_lower\n",
    "    is_undergrad = \"undergrad\" in query_lower or \"undergraduate\" in query_lower\n",
    "\n",
    "    # Extract course codes\n",
    "    course_codes = re.findall(r'\\bCS\\s*\\d{4}\\b', query.upper())\n",
    "    course_codes = [f\"CS {re.search(r'\\d{4}', code).group(0)}\" for code in course_codes]\n",
    "\n",
    "    # Handle corequisite queries\n",
    "    if \"corequisite\" in query_lower:\n",
    "        if course_codes:\n",
    "            # Find courses with specific course(s) as corequisites\n",
    "            results = []\n",
    "            for code, data in meta.items():\n",
    "                if any(c in data[\"corequisites\"] for c in course_codes):\n",
    "                    results.append((code, data))\n",
    "            return results\n",
    "        else:\n",
    "            # Find all courses with any corequisites\n",
    "            results = [(code, data) for code, data in meta.items() if data[\"corequisites\"]]\n",
    "            return results\n",
    "\n",
    "    # Handle prerequisite/dependent queries\n",
    "    if course_codes:\n",
    "        if \"prerequisite\" in query_lower:\n",
    "            if \"all\" in query_lower or \"complete list\" in query_lower or \"entire chain\" in query_lower:\n",
    "                prereqs = get_recursive_prerequisites(course_codes[0], prereq_graph)\n",
    "                return [(code, meta[code]) for code in prereqs if code in meta]\n",
    "            else:\n",
    "                prereqs = list(prereq_graph.predecessors(course_codes[0]))\n",
    "                return [(code, meta[code]) for code in prereqs if code in meta]\n",
    "        elif \"for\" in query_lower or \"before\" in query_lower or \"needed\" in query_lower:\n",
    "            prereqs = list(prereq_graph.predecessors(course_codes[0]))\n",
    "            return [(code, meta[code]) for code in prereqs if code in meta]\n",
    "        elif \"have\" in query_lower or \"require\" in query_lower or \"after\" in query_lower:\n",
    "            if len(course_codes) > 1 and \"and\" in query_lower:\n",
    "                dependents = set()\n",
    "                for course_code in course_codes:\n",
    "                    try:\n",
    "                        course_dependents = set(prereq_graph.successors(course_code))\n",
    "                        if not dependents:\n",
    "                            dependents = course_dependents\n",
    "                        else:\n",
    "                            dependents &= course_dependents\n",
    "                    except nx.NetworkXError:\n",
    "                        return []\n",
    "                return [(code, meta[code]) for code in dependents if code in meta]\n",
    "            else:\n",
    "                try:\n",
    "                    dependents = list(prereq_graph.successors(course_codes[0]))\n",
    "                    return [(code, meta[code]) for code in dependents if code in meta]\n",
    "                except nx.NetworkXError:\n",
    "                    return []\n",
    "        elif \"what comes after\" in query_lower or \"all dependents\" in query_lower:\n",
    "            dependents = get_recursive_dependents(course_codes[0], prereq_graph)\n",
    "            return [(code, meta[code]) for code in dependents if code in meta]\n",
    "\n",
    "    # Standard keyword search\n",
    "    if any(op in query.upper() for op in (\"AND\", \"OR\", \"NOT\")):\n",
    "        candidates = process_boolean_query(tokens, inv_index)\n",
    "    else:\n",
    "        candidates = set()\n",
    "        for token in tokens:\n",
    "            candidates |= inv_index.get(token, set())\n",
    "\n",
    "    if use_tfidf:\n",
    "        scores = compute_tfidf_scores(tokens, documents)\n",
    "    elif use_bm25:\n",
    "        scores = compute_bm25_scores(tokens, documents,meta)\n",
    "    else:\n",
    "        scores = {}\n",
    "\n",
    "    def filter_course(code):\n",
    "        try:\n",
    "            num = int(code.split()[1]) if code.startswith(\"CS \") else 0\n",
    "            if is_master:\n",
    "                return num >= 5000\n",
    "            elif is_align:\n",
    "                return 5000 <= num <= 5009\n",
    "            elif is_undergrad:\n",
    "                return num < 5000\n",
    "            return True\n",
    "        except (ValueError, IndexError):\n",
    "            return False\n",
    "\n",
    "    filtered_candidates = {code for code in candidates if filter_course(code)}\n",
    "    results = [(code, meta[code]) for code in scores if code in filtered_candidates] if scores else [(code, meta[code]) for code in filtered_candidates]\n",
    "\n",
    "    if pagerank:\n",
    "        for i, (code, data) in enumerate(results):\n",
    "            data[\"score\"] = scores.get(code, 0) + boost_weight * pagerank.get(code, 0)\n",
    "        results = sorted(results, key=lambda x: x[1][\"score\"], reverse=True)\n",
    "    elif scores:\n",
    "        results = sorted(results, key=lambda x: scores.get(x[0], 0), reverse=True)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(results, relevant, k=10):\n",
    "    retrieved = [r[0] for r in results[:k]]\n",
    "    return len(set(retrieved) & set(relevant)) / k\n",
    "\n",
    "def ndcg_at_k(results, relevant, k=10):\n",
    "    def dcg(rel):\n",
    "        return sum([1 / math.log2(i + 2) if rel[i] else 0 for i in range(len(rel))])\n",
    "    rels = [1 if r[0] in relevant else 0 for r in results[:k]]\n",
    "    ideal = sorted(rels, reverse=True)\n",
    "    return dcg(rels) / (dcg(ideal) or 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "\n",
    "def run_tk_gui(inv_index, docs, meta, pagerank_scores):\n",
    "    def on_search():\n",
    "        query = entry.get()\n",
    "        result_display.config(state='normal')\n",
    "        result_display.delete(\"1.0\", tk.END)\n",
    "        eval_label.config(text=\"\")\n",
    "\n",
    "        results = search(\n",
    "            query,\n",
    "            inv_index,\n",
    "            docs,\n",
    "            meta,\n",
    "            use_bm25=not tfidf_var.get(),\n",
    "            use_tfidf=tfidf_var.get(),\n",
    "            pagerank=pagerank_scores if pagerank_var.get() else None\n",
    "        )\n",
    "\n",
    "        for i, (code, data) in enumerate(results[:10], 1):\n",
    "            result_display.insert(tk.END, f\"{i}. {code} — {data['name']}\\n\")\n",
    "            result_display.insert(tk.END, f\"   Description     : {data['description']}\\n\")\n",
    "            result_display.insert(tk.END, f\"   Prerequisite(s) : {', '.join(data['prerequisites']) or 'None'}\\n\")\n",
    "            result_display.insert(tk.END, f\"   Corequisite(s)  : {', '.join(data['corequisites']) or 'None'}\\n\")\n",
    "            result_display.insert(tk.END, f\"   Hours           : {data['hours']}\\n\")\n",
    "            result_display.insert(tk.END, \"-\" * 100 + \"\\n\")\n",
    "\n",
    "        result_display.config(state='disabled')\n",
    "\n",
    "        q_clean = query.lower().strip()\n",
    "        if q_clean in query_relevance_map:\n",
    "            relevant = query_relevance_map[q_clean]\n",
    "            p10 = precision_at_k(results, relevant)\n",
    "            ndcg = ndcg_at_k(results, relevant)\n",
    "            eval_label.config(\n",
    "                text=f\"📊 Evaluation for '{q_clean}':  Precision@10 = {p10:.2f},  nDCG@10 = {ndcg:.2f}\"\n",
    "            )\n",
    "        else:\n",
    "            eval_label.config(text=\"📊 No relevance data for this query.\")\n",
    "\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Course Search Engine\")\n",
    "\n",
    "    tk.Label(root, text=\"Search Query:\").pack()\n",
    "    entry = tk.Entry(root, width=100)\n",
    "    entry.pack(pady=4)\n",
    "\n",
    "    tfidf_var = tk.BooleanVar()\n",
    "    pagerank_var = tk.BooleanVar()\n",
    "    tk.Checkbutton(root, text=\"Use TF-IDF instead of BM25\", variable=tfidf_var).pack()\n",
    "    tk.Checkbutton(root, text=\"Boost with PageRank\", variable=pagerank_var).pack()\n",
    "\n",
    "    tk.Button(root, text=\"Search\", command=on_search).pack(pady=4)\n",
    "\n",
    "    result_display = tk.Text(root, width=200, height=50, wrap=\"word\")\n",
    "    result_display.pack(padx=5, pady=5)\n",
    "    result_display.config(state='disabled')\n",
    "\n",
    "    eval_label = tk.Label(root, text=\"\", font=(\"Helvetica\", 16), fg=\"green\")\n",
    "    eval_label.pack(pady=5)\n",
    "\n",
    "    root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cli_search_interface(inv_index, docs, meta):\n",
    "    print(\"=== Course Search Engine (CLI) ===\")\n",
    "    print(\"Type 'exit' to quit.\\n\")\n",
    "    while True:\n",
    "        query = input(\"Search Query: \")\n",
    "        if query.lower() == \"exit\":\n",
    "            break\n",
    "\n",
    "        results = search(query, inv_index, docs, meta)\n",
    "\n",
    "        print(\"\\nTop Results:\")\n",
    "        for i, (code, data) in enumerate(results[:10], 1):\n",
    "            print(f\"{i}.\")\n",
    "            print(f\"Course code     : {code}\")\n",
    "            print(f\"Course name     : {data['name']}\")\n",
    "            print(f\"Description     : {data['description']}\")\n",
    "            print(f\"Prerequisite(s) : {', '.join(data['prerequisites']) or 'None'}\")\n",
    "            print(f\"Corequisite(s)  : {', '.join(data['corequisites']) or 'None'}\")\n",
    "            print(f\"Hours           : {data['hours']}\")\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "        q_clean = query.lower().strip()\n",
    "        if q_clean in query_relevance_map:\n",
    "            relevant = query_relevance_map[q_clean]\n",
    "            p10 = precision_at_k(results, relevant)\n",
    "            ndcg = ndcg_at_k(results, relevant)\n",
    "            print(f\"📊 Evaluation for '{q_clean}':  Precision@10 = {p10:.2f},  nDCG@10 = {ndcg:.2f}\")\n",
    "        else:\n",
    "            print(\"📊 No relevance data for this query.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    xml_file = \"cs_courses.xml\"\n",
    "    inv_index, docs, meta = build_index(xml_file)\n",
    "    prereq_graph = build_prerequisite_graph(\"cs_courses.xml\")\n",
    "    pagerank_scores = compute_pagerank(prereq_graph)\n",
    "\n",
    "    mode = input(\"Choose interface [cli/gui]: \").lower()\n",
    "    if mode == \"cli\":\n",
    "        cli_search_interface(inv_index, docs, meta)\n",
    "    elif mode == \"gui\":\n",
    "        run_tk_gui(inv_index, docs, meta, pagerank_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
