{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/someshb/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/someshb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import bigrams\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import nltk\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully parsed 172 courses.\n"
     ]
    }
   ],
   "source": [
    "def parse_courses(xml_content):\n",
    "    soup = BeautifulSoup(xml_content, 'lxml-xml')\n",
    "    courses = []\n",
    "    \n",
    "    for course in soup.find_all('course'):\n",
    "        code = course.find('code').text.strip().replace('\\xa0', ' ') if course.find('code') else ''\n",
    "        title = course.find('title').text.strip() if course.find('title') else ''\n",
    "        desc = course.find('description').text.strip() if course.find('description') else ''\n",
    "        level = int(course.find('level').text.strip()) if course.find('level') else 0\n",
    "        \n",
    "        prereqs = [prereq.text.strip().replace('\\xa0', ' ') for prereq in course.find_all('prerequisite')]\n",
    "        coreqs = [coreq.text.strip().replace('\\xa0', ' ') for coreq in course.find_all('corequisite')]\n",
    "        \n",
    "        hours = int(course.find('hours').text.strip()) if course.find('hours') else 0  # Check for <hours> tag\n",
    "        restrictions = course.find('restrictions').text.strip() if course.find('restrictions') else ''\n",
    "        \n",
    "        courses.append({\n",
    "            'code': code,\n",
    "            'title': title,\n",
    "            'description': desc,\n",
    "            'prerequisites': prereqs,\n",
    "            'corequisites': coreqs,\n",
    "            'level': level,\n",
    "            'hours': hours,\n",
    "            'restrictions': restrictions\n",
    "        })\n",
    "    \n",
    "    return courses\n",
    "\n",
    "try:\n",
    "    with open('courses.xml', 'r', encoding='utf-8') as file:\n",
    "        xml_content = file.read()\n",
    "    courses = parse_courses(xml_content)\n",
    "    print(f\"Successfully parsed {len(courses)} courses.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading or parsing course data: {e}\")\n",
    "    courses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prerequisites normalized and topics inferred.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def normalize_prereqs(prereqs):\n",
    "    normalized = []\n",
    "    for prereq in prereqs:\n",
    "        # Split on \"and\" or \"or\" for complex prerequisites\n",
    "        parts = re.split(r'\\s*(?:and|or)\\s*', prereq, flags=re.IGNORECASE)\n",
    "        for part in parts:\n",
    "            # Extract course codes (e.g., \"CS 3500\")\n",
    "            match = re.match(r'([A-Z]+\\s*\\d+)(?:\\s*or\\s*equivalent)?', part.strip())\n",
    "            if match:\n",
    "                normalized.append(match.group(1).replace('\\xa0', ' '))\n",
    "            else:\n",
    "                # Keep non-standard prereqs as-is (e.g., \"Permission of instructor\")\n",
    "                normalized.append(part.strip())\n",
    "    return list(dict.fromkeys(normalized))  # Remove duplicates while preserving order\n",
    "\n",
    "def infer_topics(courses):\n",
    "    topic_keywords = {\n",
    "        'ai': ['artificial', 'intelligence', 'ai', 'machine learning', 'deep learning', 'reinforcement'],\n",
    "        'nlp': ['natural language', 'nlp', 'language processing', 'speech', 'text', 'semantic'],\n",
    "        'programming': ['programming', 'coding', 'software', 'design', 'development', 'object-oriented'],\n",
    "        'systems': ['system', 'operating', 'distributed', 'network', 'cloud', 'architecture'],\n",
    "        'theory': ['theory', 'logic', 'computation', 'complexity', 'automata', 'discrete'],\n",
    "        'algorithms': ['algorithm', 'data structure', 'optimization', 'graph', 'search'],\n",
    "        'database': ['database', 'sql', 'data management', 'retrieval'],\n",
    "        'security': ['security', 'cryptography', 'privacy'],\n",
    "        'graphics': ['graphic', 'rendering', 'visualization', 'game'],\n",
    "        'hci': ['human-computer', 'interaction', 'interface', 'usability'],\n",
    "        'lab': ['lab', 'experiment', 'hands-on', 'practical'],  # Strengthened lab keywords\n",
    "        'data_science': ['data science', 'mining', 'statistics', 'predictive', 'analytics']\n",
    "    }\n",
    "    \n",
    "    for course in courses:\n",
    "        text = (course['title'] + ' ' + course['description']).lower()\n",
    "        tokens = process_query(text)\n",
    "        topics = set()\n",
    "        \n",
    "        # Keyword-based topic assignment\n",
    "        for topic, keywords in topic_keywords.items():\n",
    "            if any(kw in tokens for kw in keywords):\n",
    "                topics.add(topic)\n",
    "        \n",
    "        # Enhanced lab detection\n",
    "        if 'lab' in course['title'].lower() or 'experiments' in text or 'hands-on' in text:\n",
    "            topics.add('lab')\n",
    "        \n",
    "        # Context-based rules for electives and seminars\n",
    "        if 'elective credit' in text or 'repeated' in text or 'research' in text:\n",
    "            if not topics:\n",
    "                topics.add('misc')\n",
    "        if 'seminar' in course['title'].lower() and not topics:\n",
    "            topics.add('misc')\n",
    "        if 'directed study' in course['title'].lower() and not topics:\n",
    "            topics.add('misc')\n",
    "        \n",
    "        course['topics'] = list(topics)\n",
    "\n",
    "for course in courses:\n",
    "    course['prerequisites'] = normalize_prereqs(course['prerequisites'])\n",
    "infer_topics(courses)\n",
    "print(\"Prerequisites normalized and topics inferred.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = {\n",
    "    'ai': ['artificial', 'intelligence', 'ai'],\n",
    "    'ml': ['machine', 'learning', 'ml'],\n",
    "    'programming': ['coding', 'development', 'programming'],\n",
    "    'web': ['website', 'internet', 'web'],\n",
    "    'data': ['information', 'records', 'data'],\n",
    "    'vr': ['virtual', 'reality', 'vr', 'virtualreality'],\n",
    "    'ar': ['augmented', 'reality', 'ar'],\n",
    "    'cs': ['computer', 'science', 'cs'],\n",
    "    'it': ['information', 'technology', 'it'],\n",
    "    'cybersecurity': ['cyber', 'security', 'cybersecurity'],\n",
    "    'database': ['db', 'data', 'base'],\n",
    "    'network': ['net', 'work', 'network'],\n",
    "    'software': ['app', 'application', 'software'],\n",
    "    'ir': ['information', 'retrieval', 'ir'],\n",
    "    'hci': ['human', 'computer', 'interaction', 'hci'],\n",
    "    'graphics': ['graphic', 'design', 'graphics'],\n",
    "    'algorithms': ['algorithm', 'algorithms'],\n",
    "    'theory': ['theoretical', 'concepts', 'theory'],\n",
    "    'systems': ['system', 'infrastructure', 'systems'],\n",
    "    'nlp': ['natural', 'language', 'processing', 'nlp']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['program', 'coding', 'programming', 'development', 'intelligence', 'artificial', 'ai', 'ai program']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def process_query(query):\n",
    "    tokens = re.findall(r'\\b\\w+\\b', query.lower())\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    stemmed_tokens = [stemmer.stem(t) for t in tokens]\n",
    "    all_tokens = list(set(tokens + stemmed_tokens))\n",
    "    \n",
    "    # Add bigrams for phrase extraction\n",
    "    bi_tokens = [' '.join(b) for b in bigrams(tokens)]\n",
    "    all_tokens.extend([stemmer.stem(bt) for bt in bi_tokens])\n",
    "    \n",
    "    # Synonym Expansion\n",
    "    expanded = []\n",
    "    for t in all_tokens:\n",
    "        if t in synonyms:\n",
    "            expanded.extend(synonyms[t])\n",
    "        else:\n",
    "            expanded.append(t)\n",
    "    \n",
    "    return list(set(expanded))\n",
    "\n",
    "# Test\n",
    "print(process_query(\"AI programming\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted index built.\n"
     ]
    }
   ],
   "source": [
    "def build_inverted_index(courses):\n",
    "    index = {}\n",
    "    for course in courses:\n",
    "        text = f\"{course['code']} {course['title']} {course['description']}\".lower()\n",
    "        tokens = process_query(text)\n",
    "        for token in set(tokens):\n",
    "            if token not in index:\n",
    "                index[token] = []\n",
    "            index[token].append(course['code'])\n",
    "    return index\n",
    "\n",
    "inverted_index = build_inverted_index(courses)\n",
    "print(\"Inverted index built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 index built.\n"
     ]
    }
   ],
   "source": [
    "def build_bm25_index(courses):\n",
    "    bm25_index = defaultdict(dict)\n",
    "    doc_lengths = {}\n",
    "    df = defaultdict(int)\n",
    "    N = len(courses)\n",
    "    \n",
    "    for course in courses:\n",
    "        text = f\"{course['code']} {course['title']} {course['description']}\".lower()\n",
    "        tokens = process_query(text)\n",
    "        doc_lengths[course['code']] = len(tokens)\n",
    "        tf = defaultdict(int)\n",
    "        for token in tokens:\n",
    "            tf[token] += 1\n",
    "        for token in set(tokens):\n",
    "            df[token] += 1\n",
    "        bm25_index[course['code']] = dict(tf)\n",
    "    \n",
    "    avg_dl = sum(doc_lengths.values()) / N if N > 0 else 0\n",
    "    return bm25_index, df, doc_lengths, avg_dl\n",
    "\n",
    "bm25_index, df, doc_lengths, avg_dl = build_bm25_index(courses)\n",
    "print(\"BM25 index built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prerequisite graph and attribute index built.\n"
     ]
    }
   ],
   "source": [
    "def build_prereq_graph(courses):\n",
    "    graph = defaultdict(list)\n",
    "    code_to_idx = {c['code']: i for i, c in enumerate(courses)}\n",
    "    for i, course in enumerate(courses):\n",
    "        for prereq in course['prerequisites'] + course['corequisites']:\n",
    "            if prereq in code_to_idx:\n",
    "                graph[code_to_idx[prereq]].append(i)\n",
    "    N = len(courses)\n",
    "    pr = [1/N] * N\n",
    "    d = 0.85\n",
    "    for _ in range(20):\n",
    "        new_pr = [0] * N\n",
    "        for i in range(N):\n",
    "            inbound = graph[i]\n",
    "            new_pr[i] = (1 - d) / N + d * sum(pr[j] / len(graph[j]) for j in inbound if len(graph[j]) > 0)\n",
    "        pr = new_pr\n",
    "    return {courses[i]['code']: pr[i] for i in range(N)}\n",
    "\n",
    "prereq_rank = build_prereq_graph(courses)\n",
    "\n",
    "attribute_index = defaultdict(list)\n",
    "for course in courses:\n",
    "    attribute_index['level'].append((course['code'], course['level']))\n",
    "    attribute_index['hours'].append((course['code'], course['hours']))\n",
    "    if 'topics' in course:\n",
    "        for topic in course['topics']:\n",
    "            attribute_index[topic].append(course['code'])\n",
    "\n",
    "print(\"Prerequisite graph and attribute index built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_score(query_terms, doc_id, bm25_index, df, doc_lengths, avg_dl, k1=1.5, b=0.75):\n",
    "    score = 0.0\n",
    "    N = len(doc_lengths)\n",
    "    \n",
    "    if doc_id not in bm25_index:\n",
    "        return 0.0\n",
    "        \n",
    "    tf = bm25_index[doc_id]\n",
    "    dl = doc_lengths.get(doc_id, 0)\n",
    "    \n",
    "    for term in query_terms:\n",
    "        if term in tf:\n",
    "            idf = max(0, math.log((N - df[term] + 0.5) / (df[term] + 0.5) + 1))\n",
    "            term_score = idf * (tf[term] * (k1 + 1)) / (tf[term] + k1 * (1 - b + b * dl / avg_dl)) if avg_dl > 0 else 0\n",
    "            score += term_score\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attribute_score(course, query_terms):\n",
    "    score = 0\n",
    "    query_lower = ' '.join(query_terms).lower()\n",
    "    \n",
    "    # Boost topical relevance\n",
    "    if 'topics' in course and any(t in query_lower for t in course['topics']):\n",
    "        score += 2.0\n",
    "    # Stronger boost for \"lab\" in query and course\n",
    "    if 'lab' in query_lower and ('lab' in course['topics'] or 'lab' in course['title'].lower()):\n",
    "        score += 4.0  # Increased from 1.0 to prioritize lab courses\n",
    "    # Extra boost for NLP (from previous fix)\n",
    "    if 'topics' in course and 'nlp' in query_lower and 'nlp' in course['topics']:\n",
    "        score += 3.0\n",
    "    # Penalize generic electives and misc topics\n",
    "    if 'elective credit' in course['description'].lower() or 'misc' in course.get('topics', []):\n",
    "        if not any(t in query_lower for t in course.get('topics', [])):\n",
    "            score -= 1.0\n",
    "    if 'hours' in course and \"beginner\" in query_lower:\n",
    "        score -= course['hours'] * 0.1\n",
    "    if \"masters\" in query_lower and course['level'] >= 5000:\n",
    "        score += 1.0\n",
    "    return score\n",
    "\n",
    "def pagerank_score(course):\n",
    "    return prereq_rank.get(course['code'], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_filters(results, query):\n",
    "    query_lower = query.lower()\n",
    "    \n",
    "    not_masters = any(pattern in query_lower for pattern in [\"not master\", \"not graduate\", \"no master\", \"no graduate\"])\n",
    "    not_undergrad = any(pattern in query_lower for pattern in [\"not undergrad\", \"not undergraduate\", \"no undergrad\", \"no undergraduate\"])\n",
    "    \n",
    "    level_filters = {}\n",
    "    if \"master\" in query_lower or \"masters\" in query_lower or \"master's\" in query_lower or \"graduate\" in query_lower:\n",
    "        if not not_masters:\n",
    "            level_filters[\"masters\"] = lambda c: c['level'] >= 5000\n",
    "    if \"undergrad\" in query_lower or \"undergraduate\" in query_lower:\n",
    "        if not not_undergrad:\n",
    "            level_filters[\"undergraduate\"] = lambda c: c['level'] < 5000\n",
    "    if not_masters:\n",
    "        level_filters[\"not_masters\"] = lambda c: c['level'] < 5000\n",
    "    if not_undergrad:\n",
    "        level_filters[\"not_undergrad\"] = lambda c: c['level'] >= 5000\n",
    "    \n",
    "    name_filters = {\n",
    "        \"advanced\": lambda c: \"advanced\" in c['title'].lower() or \"advanced\" in c['description'].lower(),\n",
    "        \"intro\": lambda c: any(word in c['title'].lower() for word in [\"intro\", \"introduction\", \"introductory\"]) or \n",
    "                         any(word in c['description'].lower() for word in [\"intro\", \"introduction\", \"introductory\"]),\n",
    "        \"introductory\": lambda c: any(word in c['title'].lower() for word in [\"intro\", \"introduction\", \"introductory\"]) or \n",
    "                               any(word in c['description'].lower() for word in [\"intro\", \"introduction\", \"introductory\"]),\n",
    "        \"beginner\": lambda c: any(word in c['title'].lower() for word in [\"beginner\", \"beginning\", \"elementary\", \"fundamental\"]) or\n",
    "                           any(word in c['description'].lower() for word in [\"beginner\", \"beginning\", \"elementary\", \"fundamental\"]),\n",
    "        \"hands-on\": lambda c: \"lab\" in c['description'].lower() or \"practical\" in c['description'].lower(),\n",
    "        \"lab\": lambda c: \"lab\" in c['description'].lower()\n",
    "    }\n",
    "    \n",
    "    all_filters = {**level_filters, **name_filters}\n",
    "    active_filters = []\n",
    "    filter_terms = set(name_filters.keys()) | {\"master\", \"masters\", \"master's\", \"graduate\", \"undergrad\", \"undergraduate\", \"not\", \"no\"}\n",
    "    \n",
    "    clean_query = query_lower\n",
    "    for term in filter_terms:\n",
    "        clean_query = re.sub(r'\\b' + re.escape(term) + r'\\b', '', clean_query)\n",
    "    clean_query = re.sub(r'not\\s+\\w+', '', clean_query)\n",
    "    clean_query = re.sub(r'no\\s+\\w+', '', clean_query)\n",
    "    clean_query = re.sub(r'\\s+', ' ', clean_query).strip()\n",
    "    \n",
    "    for filter_func in level_filters.values():\n",
    "        active_filters.append(filter_func)\n",
    "    for term, filter_func in name_filters.items():\n",
    "        if term in query_lower:\n",
    "            active_filters.append(filter_func)\n",
    "    \n",
    "    if not active_filters:\n",
    "        return results, clean_query\n",
    "    \n",
    "    filtered_results = []\n",
    "    for course in results:\n",
    "        if all(f(course) for f in active_filters):\n",
    "            filtered_results.append(course)\n",
    "    \n",
    "    return filtered_results, clean_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_courses(query, index_method='Both'):\n",
    "    query = query.strip()\n",
    "    if not query:\n",
    "        return []\n",
    "    \n",
    "    results = courses\n",
    "    results, clean_query = apply_filters(results, query)\n",
    "    \n",
    "    if not clean_query.strip():\n",
    "        return results\n",
    "    \n",
    "    processed_terms = process_query(clean_query)\n",
    "    if not processed_terms:\n",
    "        return results\n",
    "    \n",
    "    combined_scores = defaultdict(float)\n",
    "    \n",
    "    if index_method in ['Inverted Index', 'Both']:\n",
    "        doc_codes = set()\n",
    "        for term in processed_terms:\n",
    "            if term in inverted_index:\n",
    "                doc_codes.update(inverted_index[term])\n",
    "        filtered_codes = {course['code'] for course in results}\n",
    "        doc_codes = doc_codes.intersection(filtered_codes)\n",
    "        for code in doc_codes:\n",
    "            combined_scores[code] += 1.0\n",
    "    \n",
    "    if index_method in ['BM25', 'Both']:\n",
    "        for course in results:\n",
    "            code = course['code']\n",
    "            score = bm25_score(processed_terms, code, bm25_index, df, doc_lengths, avg_dl)\n",
    "            if score > 0:\n",
    "                combined_scores[code] += min(5.0, score)\n",
    "    \n",
    "    if combined_scores:\n",
    "        course_dict = {c['code']: c for c in results}\n",
    "        scored_courses = []\n",
    "        for code, score in combined_scores.items():\n",
    "            if code in course_dict:\n",
    "                course = course_dict[code]\n",
    "                pr_score = pagerank_score(course)\n",
    "                attr_score = attribute_score(course, processed_terms)\n",
    "                final_score = (0.4 * score) + (0.5 * attr_score) + (0.1 * pr_score)  # Adjusted: attr > BM25\n",
    "                scored_courses.append((course, final_score))\n",
    "        \n",
    "        scored_courses.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [s[0] for s in scored_courses]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sz/3_3bz7f50bs3023_2hj47k5h0000gn/T/ipykernel_85203/1098046052.py:64: DeprecationWarning: on_submit is deprecated. Instead, set the .continuous_update attribute to False and observe the value changing with: mywidget.observe(callback, 'value').\n",
      "  query_input.on_submit(lambda widget: on_submit(None))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b5860fa15974ae1bf225290137c32d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Query:', placeholder='Enter your query...')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d834dcdaf9943c48ce062b8a2d7658f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='Index:', index=2, options=('Inverted Index', 'BM25', 'Both'), value='Both')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9def10d9f80548e0b9b49c9bc818b92a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=10, description='Max Results:', max=50, min=5, step=5)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbee75d92aa54ca991168c40d7217ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Search', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d15d99fab0ca429c9e855676838197c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "annotated_queries = {\n",
    "    \"courses with AI\": [\"CS 5100\", \"CS 4050\", \"CS 5047\", \"CS 7170\", \"CS 4100\", \"CS 4150\", \"CS 5150\"],\n",
    "    \"intro programming\": [\"CS 2500\", \"CS 2510\"],\n",
    "    \"NLP courses\": [\"CS 6120\", \"CS 4120\", \"CS 5100\", \"CS 7150\", \"CS 4100\"],\n",
    "    \"courses with Lab\": [\"CS 1101\", \"CS 2501\", \"CS 2511\", \"CS 3501\", \"CS 5003\"],\n",
    "    \"systems courses\": [\"CS 3650\", \"CS 3700\", \"CS 5600\", \"CS 5610\", \"CS 6240\"],\n",
    "    \"theory courses\": [\"CS 1800\", \"CS 2800\", \"CS 3800\", \"CS 4800\", \"CS 5800\"],\n",
    "    \"database courses\": [\"CS 3200\", \"CS 5200\", \"CS 6200\", \"CS 7200\"],\n",
    "    \"security courses\": [\"CS 3740\", \"CS 4740\", \"CS 5770\", \"CS 6740\"],\n",
    "    \"graphics courses\": [\"CS 4300\", \"CS 4360\", \"CS 5360\", \"CS 5540\"],\n",
    "    \"advanced algorithms\": [\"CS 5800\", \"CS 7800\", \"CS 6810\"],\n",
    "    \"data science\": [\"CS 6220\", \"CS 6140\", \"CS 7290\", \"DS 5220\"],\n",
    "    \"human-computer interaction\": [\"CS 5340\", \"CS 6330\", \"CS 7340\"],\n",
    "    \"beginner cs\": [\"CS 1100\", \"CS 1200\", \"CS 2500\", \"CS 2510\"],\n",
    "    \"masters level\": [\"CS 5100\", \"CS 5200\", \"CS 5600\", \"CS 5800\", \"CS 6140\"],\n",
    "    \"game development\": [\"CS 5150\", \"CS 5540\", \"CS 5850\"]\n",
    "}\n",
    "\n",
    "def evaluate(query, results):\n",
    "    true_relevance = [1 if r['code'] in annotated_queries.get(query, []) else 0 for r in results]\n",
    "    pred_relevance = [1/i for i in range(1, len(results)+1)]\n",
    "    ndcg = ndcg_score([true_relevance], [pred_relevance]) if true_relevance else 0\n",
    "    precision_at_10 = sum(true_relevance[:10]) / min(10, len(results)) if results else 0\n",
    "    return ndcg, precision_at_10\n",
    "\n",
    "query_input = widgets.Text(placeholder='Enter your query...', description='Query:', disabled=False)\n",
    "index_selector = widgets.RadioButtons(options=['Inverted Index', 'BM25', 'Both'], description='Index:', value='Both')\n",
    "max_results = widgets.IntSlider(value=10, min=5, max=50, step=5, description='Max Results:')\n",
    "submit_button = widgets.Button(description='Search')\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_submit(b):\n",
    "    query = query_input.value\n",
    "    results = search_courses(query, index_selector.value)\n",
    "    \n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        print(f\"Results for query: '{query}'\")\n",
    "        \n",
    "        if not results:\n",
    "            print(\"No matching courses found.\")\n",
    "        else:\n",
    "            limit = min(len(results), max_results.value)\n",
    "            for i, course in enumerate(results[:limit]):\n",
    "                print(f\"{i+1}. {course['code']}: {course['title']}\")\n",
    "                if course['prerequisites']:\n",
    "                    print(f\"   Prerequisites: {', '.join(course['prerequisites'])}\")\n",
    "                if course['corequisites']:\n",
    "                    print(f\"   Corequisites: {', '.join(course['corequisites'])}\")\n",
    "                print(f\"   Level: {course['level']}\")\n",
    "                print(f\"   Hours: {course['hours']}\")\n",
    "                desc = course['description']\n",
    "                print(f\"   {desc[:100] + '...' if len(desc) > 100 else desc}\")\n",
    "                if 'topics' in course:\n",
    "                    print(f\"   Topics: {', '.join(course['topics'])}\")\n",
    "                print()\n",
    "            ndcg, precision = evaluate(query, results[:limit])\n",
    "            print(f\"NDCG@10: {ndcg:.3f}\")\n",
    "            print(f\"Precision@10: {precision:.3f}\")\n",
    "\n",
    "submit_button.on_click(on_submit)\n",
    "query_input.on_submit(lambda widget: on_submit(None))\n",
    "display(query_input, index_selector, max_results, submit_button, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inter-Annotator Agreement (Cohen's Kappa):\n",
      "courses with AI: 1.000\n",
      "intro programming: 1.000\n",
      "NLP courses: 1.000\n",
      "courses with Lab: 1.000\n",
      "systems courses: 1.000\n",
      "theory courses: 1.000\n",
      "database courses: 0.854\n",
      "security courses: nan\n",
      "graphics courses: 1.000\n",
      "advanced algorithms: 1.000\n",
      "data science: 1.000\n",
      "human-computer interaction: 1.000\n",
      "beginner cs: 1.000\n",
      "masters level: 1.000\n",
      "game development: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ML/lib/python3.12/site-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/ML/lib/python3.12/site-packages/sklearn/metrics/_classification.py:758: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  k = np.sum(w_mat * confusion) / np.sum(w_mat * expected)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "import random\n",
    "\n",
    "# Simulate two annotators with slight disagreements\n",
    "annotator1 = annotated_queries.copy()\n",
    "annotator2 = annotated_queries.copy()\n",
    "\n",
    "# Introduce some random disagreements for realism\n",
    "for query in annotator2:\n",
    "    if random.random() < 0.2:  # 20% chance of disagreement\n",
    "        if annotator2[query]:\n",
    "            annotator2[query] = annotator2[query][:-1]  # Remove one course\n",
    "        else:\n",
    "            annotator2[query].append(\"CS 9999\")  # Add a fake course\n",
    "\n",
    "def inter_annotator_agreement(query, annotator1, annotator2, all_courses):\n",
    "    codes = set(c['code'] for c in all_courses)\n",
    "    a1_labels = [1 if c in annotator1.get(query, []) else 0 for c in codes]\n",
    "    a2_labels = [1 if c in annotator2.get(query, []) else 0 for c in codes]\n",
    "    return cohen_kappa_score(a1_labels, a2_labels)\n",
    "\n",
    "# Compute agreement for all queries\n",
    "print(\"Inter-Annotator Agreement (Cohen's Kappa):\")\n",
    "for query in annotated_queries:\n",
    "    kappa = inter_annotator_agreement(query, annotator1, annotator2, courses)\n",
    "    print(f\"{query}: {kappa:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: proof, functions, inductive, 5010, structures\n",
      "Topic 1: player, artificial, services, intelligence, dimensional\n",
      "Topic 2: offering, formal, object, format, cs\n",
      "Topic 3: web, mobile, focuses, material, student\n",
      "Topic 4: natural, game, language, artificial, intelligence\n",
      "Topic 5: visualization, research, doctoral, work, experience\n",
      "Topic 6: credit, academic, institutions, taken, elective\n",
      "Topic 7: science, students, systems, topics, computer\n",
      "Topic 8: work, selected, thesis, supervisor, agreement\n",
      "Topic 9: testing, software, algorithms, complexity, design\n",
      "TF-IDF/LDA topics assigned to courses.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "\n",
    "def tfidf_lda_topic_tagging(courses, num_topics=10):\n",
    "    # Combine title and description for each course\n",
    "    documents = [f\"{c['title']} {c['description']}\" for c in courses]\n",
    "    \n",
    "    # TF-IDF Vectorization\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # LDA Topic Modeling\n",
    "    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "    lda_matrix = lda.fit_transform(tfidf_matrix)\n",
    "    \n",
    "    # Assign topics to courses\n",
    "    topic_names = [f\"topic_{i}\" for i in range(num_topics)]\n",
    "    for i, course in enumerate(courses):\n",
    "        topic_dist = lda_matrix[i]\n",
    "        top_topic_idx = np.argmax(topic_dist)\n",
    "        course['topics'] = [topic_names[top_topic_idx]]\n",
    "    \n",
    "    # Print top words for each topic (for interpretation)\n",
    "    for i, topic in enumerate(lda.components_):\n",
    "        top_words = [feature_names[j] for j in topic.argsort()[-5:]]\n",
    "        print(f\"Topic {i}: {', '.join(top_words)}\")\n",
    "\n",
    "# Replace rule-based topics with TF-IDF/LDA\n",
    "tfidf_lda_topic_tagging(courses)\n",
    "print(\"TF-IDF/LDA topics assigned to courses.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
