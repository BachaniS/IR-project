{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/someshb/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/someshb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import bigrams\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import nltk\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully parsed 172 courses.\n"
     ]
    }
   ],
   "source": [
    "def parse_courses(xml_content):\n",
    "    soup = BeautifulSoup(xml_content, 'lxml-xml')\n",
    "    courses = []\n",
    "    \n",
    "    for course in soup.find_all('course'):\n",
    "        code = course.find('code').text.strip().replace('\\xa0', ' ') if course.find('code') else ''\n",
    "        title = course.find('title').text.strip() if course.find('title') else ''\n",
    "        desc = course.find('description').text.strip() if course.find('description') else ''\n",
    "        level = int(course.find('level').text.strip()) if course.find('level') else 0\n",
    "        \n",
    "        prereqs = [prereq.text.strip().replace('\\xa0', ' ') for prereq in course.find_all('prerequisite')]\n",
    "        coreqs = [coreq.text.strip().replace('\\xa0', ' ') for coreq in course.find_all('corequisite')]\n",
    "        \n",
    "        hours = int(course.find('hours').text.strip()) if course.find('hours') else 0  # Check for <hours> tag\n",
    "        restrictions = course.find('restrictions').text.strip() if course.find('restrictions') else ''\n",
    "        \n",
    "        courses.append({\n",
    "            'code': code,\n",
    "            'title': title,\n",
    "            'description': desc,\n",
    "            'prerequisites': prereqs,\n",
    "            'corequisites': coreqs,\n",
    "            'level': level,\n",
    "            'hours': hours,\n",
    "            'restrictions': restrictions\n",
    "        })\n",
    "    \n",
    "    return courses\n",
    "\n",
    "try:\n",
    "    with open('courses.xml', 'r', encoding='utf-8') as file:\n",
    "        xml_content = file.read()\n",
    "    courses = parse_courses(xml_content)\n",
    "    print(f\"Successfully parsed {len(courses)} courses.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading or parsing course data: {e}\")\n",
    "    courses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prerequisites normalized and topics inferred.\n"
     ]
    }
   ],
   "source": [
    "def normalize_prereqs(prereqs):\n",
    "    normalized = []\n",
    "    for prereq in prereqs:\n",
    "        match = re.match(r'([A-Z]+\\s*\\d+)(?:\\s*or\\s*equivalent)?', prereq)\n",
    "        normalized.append(match.group(1) if match else prereq)\n",
    "    return normalized\n",
    "\n",
    "def infer_topics(courses):\n",
    "    topic_keywords = {\n",
    "        'ai': ['artificial', 'intelligence', 'ai', 'machine learning', 'deep learning', 'reinforcement'],\n",
    "        'nlp': ['natural language', 'nlp', 'language processing', 'speech', 'text', 'semantic'],\n",
    "        'programming': ['programming', 'coding', 'software', 'design', 'development', 'object-oriented'],\n",
    "        'systems': ['system', 'operating', 'distributed', 'network', 'cloud', 'architecture'],\n",
    "        'theory': ['theory', 'logic', 'computation', 'complexity', 'automata', 'discrete'],\n",
    "        'algorithms': ['algorithm', 'data structure', 'optimization', 'graph', 'search'],\n",
    "        'database': ['database', 'sql', 'data management', 'retrieval'],\n",
    "        'security': ['security', 'cryptography', 'privacy'],\n",
    "        'graphics': ['graphic', 'rendering', 'visualization', 'game'],\n",
    "        'hci': ['human-computer', 'interaction', 'interface', 'usability'],\n",
    "        'lab': ['lab', 'experiment', 'hands-on', 'practical'],  # Strengthened lab keywords\n",
    "        'data_science': ['data science', 'mining', 'statistics', 'predictive', 'analytics']\n",
    "    }\n",
    "    \n",
    "    for course in courses:\n",
    "        text = (course['title'] + ' ' + course['description']).lower()\n",
    "        tokens = process_query(text)\n",
    "        topics = set()\n",
    "        \n",
    "        # Keyword-based topic assignment\n",
    "        for topic, keywords in topic_keywords.items():\n",
    "            if any(kw in tokens for kw in keywords):\n",
    "                topics.add(topic)\n",
    "        \n",
    "        # Enhanced lab detection\n",
    "        if 'lab' in course['title'].lower() or 'experiments' in text or 'hands-on' in text:\n",
    "            topics.add('lab')\n",
    "        \n",
    "        # Context-based rules for electives and seminars\n",
    "        if 'elective credit' in text or 'repeated' in text or 'research' in text:\n",
    "            if not topics:\n",
    "                topics.add('misc')\n",
    "        if 'seminar' in course['title'].lower() and not topics:\n",
    "            topics.add('misc')\n",
    "        if 'directed study' in course['title'].lower() and not topics:\n",
    "            topics.add('misc')\n",
    "        \n",
    "        course['topics'] = list(topics)\n",
    "\n",
    "for course in courses:\n",
    "    course['prerequisites'] = normalize_prereqs(course['prerequisites'])\n",
    "infer_topics(courses)\n",
    "print(\"Prerequisites normalized and topics inferred.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = {\n",
    "    'ai': ['artificial', 'intelligence', 'ai'],\n",
    "    'ml': ['machine', 'learning', 'ml'],\n",
    "    'programming': ['coding', 'development', 'programming'],\n",
    "    'web': ['website', 'internet', 'web'],\n",
    "    'data': ['information', 'records', 'data'],\n",
    "    'vr': ['virtual', 'reality', 'vr', 'virtualreality'],\n",
    "    'ar': ['augmented', 'reality', 'ar'],\n",
    "    'cs': ['computer', 'science', 'cs'],\n",
    "    'it': ['information', 'technology', 'it'],\n",
    "    'cybersecurity': ['cyber', 'security', 'cybersecurity'],\n",
    "    'database': ['db', 'data', 'base'],\n",
    "    'network': ['net', 'work', 'network'],\n",
    "    'software': ['app', 'application', 'software'],\n",
    "    'ir': ['information', 'retrieval', 'ir'],\n",
    "    'hci': ['human', 'computer', 'interaction', 'hci'],\n",
    "    'graphics': ['graphic', 'design', 'graphics'],\n",
    "    'algorithms': ['algorithm', 'algorithms'],\n",
    "    'theory': ['theoretical', 'concepts', 'theory'],\n",
    "    'systems': ['system', 'infrastructure', 'systems'],\n",
    "    'nlp': ['natural', 'language', 'processing', 'nlp']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['artificial', 'programming', 'program', 'development', 'ai program', 'intelligence', 'coding', 'ai']\n"
     ]
    }
   ],
   "source": [
    "def process_query(query):\n",
    "    tokens = re.findall(r'\\b\\w+\\b', query.lower())\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    stemmed_tokens = [stemmer.stem(t) for t in tokens]\n",
    "    all_tokens = list(set(tokens + stemmed_tokens))\n",
    "    \n",
    "    # Add bigrams for phrase extraction\n",
    "    bi_tokens = [' '.join(b) for b in bigrams(tokens)]\n",
    "    all_tokens.extend([stemmer.stem(bt) for bt in bi_tokens])\n",
    "    \n",
    "    # Synonym Expansion\n",
    "    expanded = []\n",
    "    for t in all_tokens:\n",
    "        if t in synonyms:\n",
    "            expanded.extend(synonyms[t])\n",
    "        else:\n",
    "            expanded.append(t)\n",
    "    \n",
    "    return list(set(expanded))\n",
    "\n",
    "# Test\n",
    "print(process_query(\"AI programming\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted index built.\n"
     ]
    }
   ],
   "source": [
    "def build_inverted_index(courses):\n",
    "    index = {}\n",
    "    for course in courses:\n",
    "        text = f\"{course['code']} {course['title']} {course['description']}\".lower()\n",
    "        tokens = process_query(text)\n",
    "        for token in set(tokens):\n",
    "            if token not in index:\n",
    "                index[token] = []\n",
    "            index[token].append(course['code'])\n",
    "    return index\n",
    "\n",
    "inverted_index = build_inverted_index(courses)\n",
    "print(\"Inverted index built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 index built.\n"
     ]
    }
   ],
   "source": [
    "def build_bm25_index(courses):\n",
    "    bm25_index = defaultdict(dict)\n",
    "    doc_lengths = {}\n",
    "    df = defaultdict(int)\n",
    "    N = len(courses)\n",
    "    \n",
    "    for course in courses:\n",
    "        text = f\"{course['code']} {course['title']} {course['description']}\".lower()\n",
    "        tokens = process_query(text)\n",
    "        doc_lengths[course['code']] = len(tokens)\n",
    "        tf = defaultdict(int)\n",
    "        for token in tokens:\n",
    "            tf[token] += 1\n",
    "        for token in set(tokens):\n",
    "            df[token] += 1\n",
    "        bm25_index[course['code']] = dict(tf)\n",
    "    \n",
    "    avg_dl = sum(doc_lengths.values()) / N if N > 0 else 0\n",
    "    return bm25_index, df, doc_lengths, avg_dl\n",
    "\n",
    "bm25_index, df, doc_lengths, avg_dl = build_bm25_index(courses)\n",
    "print(\"BM25 index built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prerequisite graph and attribute index built.\n"
     ]
    }
   ],
   "source": [
    "def build_prereq_graph(courses):\n",
    "    graph = defaultdict(list)\n",
    "    code_to_idx = {c['code']: i for i, c in enumerate(courses)}\n",
    "    for i, course in enumerate(courses):\n",
    "        for prereq in course['prerequisites'] + course['corequisites']:\n",
    "            if prereq in code_to_idx:\n",
    "                graph[code_to_idx[prereq]].append(i)\n",
    "    N = len(courses)\n",
    "    pr = [1/N] * N\n",
    "    d = 0.85\n",
    "    for _ in range(20):\n",
    "        new_pr = [0] * N\n",
    "        for i in range(N):\n",
    "            inbound = graph[i]\n",
    "            new_pr[i] = (1 - d) / N + d * sum(pr[j] / len(graph[j]) for j in inbound if len(graph[j]) > 0)\n",
    "        pr = new_pr\n",
    "    return {courses[i]['code']: pr[i] for i in range(N)}\n",
    "\n",
    "prereq_rank = build_prereq_graph(courses)\n",
    "\n",
    "attribute_index = defaultdict(list)\n",
    "for course in courses:\n",
    "    attribute_index['level'].append((course['code'], course['level']))\n",
    "    attribute_index['hours'].append((course['code'], course['hours']))\n",
    "    if 'topics' in course:\n",
    "        for topic in course['topics']:\n",
    "            attribute_index[topic].append(course['code'])\n",
    "\n",
    "print(\"Prerequisite graph and attribute index built.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_score(query_terms, doc_id, bm25_index, df, doc_lengths, avg_dl, k1=1.5, b=0.75):\n",
    "    score = 0.0\n",
    "    N = len(doc_lengths)\n",
    "    \n",
    "    if doc_id not in bm25_index:\n",
    "        return 0.0\n",
    "        \n",
    "    tf = bm25_index[doc_id]\n",
    "    dl = doc_lengths.get(doc_id, 0)\n",
    "    \n",
    "    for term in query_terms:\n",
    "        if term in tf:\n",
    "            idf = max(0, math.log((N - df[term] + 0.5) / (df[term] + 0.5) + 1))\n",
    "            term_score = idf * (tf[term] * (k1 + 1)) / (tf[term] + k1 * (1 - b + b * dl / avg_dl)) if avg_dl > 0 else 0\n",
    "            score += term_score\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attribute_score(course, query_terms):\n",
    "    score = 0\n",
    "    query_lower = ' '.join(query_terms).lower()\n",
    "    \n",
    "    # Boost topical relevance\n",
    "    if 'topics' in course and any(t in query_lower for t in course['topics']):\n",
    "        score += 2.0\n",
    "    # Stronger boost for \"lab\" in query and course\n",
    "    if 'lab' in query_lower and ('lab' in course['topics'] or 'lab' in course['title'].lower()):\n",
    "        score += 4.0  # Increased from 1.0 to prioritize lab courses\n",
    "    # Extra boost for NLP (from previous fix)\n",
    "    if 'topics' in course and 'nlp' in query_lower and 'nlp' in course['topics']:\n",
    "        score += 3.0\n",
    "    # Penalize generic electives and misc topics\n",
    "    if 'elective credit' in course['description'].lower() or 'misc' in course.get('topics', []):\n",
    "        if not any(t in query_lower for t in course.get('topics', [])):\n",
    "            score -= 1.0\n",
    "    if 'hours' in course and \"beginner\" in query_lower:\n",
    "        score -= course['hours'] * 0.1\n",
    "    if \"masters\" in query_lower and course['level'] >= 5000:\n",
    "        score += 1.0\n",
    "    return score\n",
    "\n",
    "def pagerank_score(course):\n",
    "    return prereq_rank.get(course['code'], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_filters(results, query):\n",
    "    query_lower = query.lower()\n",
    "    \n",
    "    not_masters = any(pattern in query_lower for pattern in [\"not master\", \"not graduate\", \"no master\", \"no graduate\"])\n",
    "    not_undergrad = any(pattern in query_lower for pattern in [\"not undergrad\", \"not undergraduate\", \"no undergrad\", \"no undergraduate\"])\n",
    "    \n",
    "    level_filters = {}\n",
    "    if \"master\" in query_lower or \"masters\" in query_lower or \"master's\" in query_lower or \"graduate\" in query_lower:\n",
    "        if not not_masters:\n",
    "            level_filters[\"masters\"] = lambda c: c['level'] >= 5000\n",
    "    if \"undergrad\" in query_lower or \"undergraduate\" in query_lower:\n",
    "        if not not_undergrad:\n",
    "            level_filters[\"undergraduate\"] = lambda c: c['level'] < 5000\n",
    "    if not_masters:\n",
    "        level_filters[\"not_masters\"] = lambda c: c['level'] < 5000\n",
    "    if not_undergrad:\n",
    "        level_filters[\"not_undergrad\"] = lambda c: c['level'] >= 5000\n",
    "    \n",
    "    name_filters = {\n",
    "        \"advanced\": lambda c: \"advanced\" in c['title'].lower() or \"advanced\" in c['description'].lower(),\n",
    "        \"intro\": lambda c: any(word in c['title'].lower() for word in [\"intro\", \"introduction\", \"introductory\"]) or \n",
    "                         any(word in c['description'].lower() for word in [\"intro\", \"introduction\", \"introductory\"]),\n",
    "        \"introductory\": lambda c: any(word in c['title'].lower() for word in [\"intro\", \"introduction\", \"introductory\"]) or \n",
    "                               any(word in c['description'].lower() for word in [\"intro\", \"introduction\", \"introductory\"]),\n",
    "        \"beginner\": lambda c: any(word in c['title'].lower() for word in [\"beginner\", \"beginning\", \"elementary\", \"fundamental\"]) or\n",
    "                           any(word in c['description'].lower() for word in [\"beginner\", \"beginning\", \"elementary\", \"fundamental\"]),\n",
    "        \"hands-on\": lambda c: \"lab\" in c['description'].lower() or \"practical\" in c['description'].lower(),\n",
    "        \"lab\": lambda c: \"lab\" in c['description'].lower()\n",
    "    }\n",
    "    \n",
    "    all_filters = {**level_filters, **name_filters}\n",
    "    active_filters = []\n",
    "    filter_terms = set(name_filters.keys()) | {\"master\", \"masters\", \"master's\", \"graduate\", \"undergrad\", \"undergraduate\", \"not\", \"no\"}\n",
    "    \n",
    "    clean_query = query_lower\n",
    "    for term in filter_terms:\n",
    "        clean_query = re.sub(r'\\b' + re.escape(term) + r'\\b', '', clean_query)\n",
    "    clean_query = re.sub(r'not\\s+\\w+', '', clean_query)\n",
    "    clean_query = re.sub(r'no\\s+\\w+', '', clean_query)\n",
    "    clean_query = re.sub(r'\\s+', ' ', clean_query).strip()\n",
    "    \n",
    "    for filter_func in level_filters.values():\n",
    "        active_filters.append(filter_func)\n",
    "    for term, filter_func in name_filters.items():\n",
    "        if term in query_lower:\n",
    "            active_filters.append(filter_func)\n",
    "    \n",
    "    if not active_filters:\n",
    "        return results, clean_query\n",
    "    \n",
    "    filtered_results = []\n",
    "    for course in results:\n",
    "        if all(f(course) for f in active_filters):\n",
    "            filtered_results.append(course)\n",
    "    \n",
    "    return filtered_results, clean_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_courses(query, index_method='Both'):\n",
    "    query = query.strip()\n",
    "    if not query:\n",
    "        return []\n",
    "    \n",
    "    results = courses\n",
    "    results, clean_query = apply_filters(results, query)\n",
    "    \n",
    "    if not clean_query.strip():\n",
    "        return results\n",
    "    \n",
    "    processed_terms = process_query(clean_query)\n",
    "    if not processed_terms:\n",
    "        return results\n",
    "    \n",
    "    combined_scores = defaultdict(float)\n",
    "    \n",
    "    if index_method in ['Inverted Index', 'Both']:\n",
    "        doc_codes = set()\n",
    "        for term in processed_terms:\n",
    "            if term in inverted_index:\n",
    "                doc_codes.update(inverted_index[term])\n",
    "        filtered_codes = {course['code'] for course in results}\n",
    "        doc_codes = doc_codes.intersection(filtered_codes)\n",
    "        for code in doc_codes:\n",
    "            combined_scores[code] += 1.0\n",
    "    \n",
    "    if index_method in ['BM25', 'Both']:\n",
    "        for course in results:\n",
    "            code = course['code']\n",
    "            score = bm25_score(processed_terms, code, bm25_index, df, doc_lengths, avg_dl)\n",
    "            if score > 0:\n",
    "                combined_scores[code] += min(5.0, score)\n",
    "    \n",
    "    if combined_scores:\n",
    "        course_dict = {c['code']: c for c in results}\n",
    "        scored_courses = []\n",
    "        for code, score in combined_scores.items():\n",
    "            if code in course_dict:\n",
    "                course = course_dict[code]\n",
    "                pr_score = pagerank_score(course)\n",
    "                attr_score = attribute_score(course, processed_terms)\n",
    "                final_score = (0.4 * score) + (0.5 * attr_score) + (0.1 * pr_score)  # Adjusted: attr > BM25\n",
    "                scored_courses.append((course, final_score))\n",
    "        \n",
    "        scored_courses.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [s[0] for s in scored_courses]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sz/3_3bz7f50bs3023_2hj47k5h0000gn/T/ipykernel_69835/624467245.py:48: DeprecationWarning: on_submit is deprecated. Instead, set the .continuous_update attribute to False and observe the value changing with: mywidget.observe(callback, 'value').\n",
      "  query_input.on_submit(lambda widget: on_submit(None))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86495d4b46cb4e19badd56551c4c32a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Query:', placeholder='Enter your query...')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8f999cb9d7e4174bf4c06502ebf5f56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='Index:', index=2, options=('Inverted Index', 'BM25', 'Both'), value='Both')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "392fb48ff2a54ab38b4af5f26f2574b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=10, description='Max Results:', max=50, min=5, step=5)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f5914937dae498ea8cfb9822b80bcbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Search', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52d09851c80c420a9a7f000371ebd017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "annotated_queries = {\n",
    "    \"courses with AI\": [\"CS 5100\", \"CS 4050\", \"CS 5047\", \"CS 7170\", \"CS 4100\", \"CS 4150\", \"CS 5150\"],\n",
    "    \"intro programming\": [\"CS 2500\", \"CS 2510\"],\n",
    "    \"NLP courses\": [\"CS 6120\", \"CS 4120\", \"CS 5100\", \"CS 7150\", \"CS 4100\"],\n",
    "    \"courses with Lab\": [\"CS 1101\", \"CS 2501\", \"CS 2511\", \"CS 3501\", \"CS 5003\"]  # Known lab courses\n",
    "}\n",
    "\n",
    "def evaluate(query, results):\n",
    "    true_relevance = [1 if r['code'] in annotated_queries.get(query, []) else 0 for r in results]\n",
    "    pred_relevance = [1/i for i in range(1, len(results)+1)]\n",
    "    return ndcg_score([true_relevance], [pred_relevance]) if true_relevance else 0\n",
    "\n",
    "query_input = widgets.Text(placeholder='Enter your query...', description='Query:', disabled=False)\n",
    "index_selector = widgets.RadioButtons(options=['Inverted Index', 'BM25', 'Both'], description='Index:', value='Both')\n",
    "max_results = widgets.IntSlider(value=10, min=5, max=50, step=5, description='Max Results:')\n",
    "submit_button = widgets.Button(description='Search')\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_submit(b):\n",
    "    query = query_input.value\n",
    "    results = search_courses(query, index_selector.value)\n",
    "    \n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        print(f\"Results for query: '{query}'\")\n",
    "        \n",
    "        if not results:\n",
    "            print(\"No matching courses found.\")\n",
    "        else:\n",
    "            limit = min(len(results), max_results.value)\n",
    "            for i, course in enumerate(results[:limit]):\n",
    "                print(f\"{i+1}. {course['code']}: {course['title']}\")\n",
    "                if course['prerequisites']:\n",
    "                    print(f\"   Prerequisites: {', '.join(course['prerequisites'])}\")\n",
    "                if course['corequisites']:\n",
    "                    print(f\"   Corequisites: {', '.join(course['corequisites'])}\")\n",
    "                print(f\"   Level: {course['level']}\")\n",
    "                print(f\"   Hours: {course['hours']}\")\n",
    "                desc = course['description']\n",
    "                print(f\"   {desc[:100] + '...' if len(desc) > 100 else desc}\")\n",
    "                if 'topics' in course:\n",
    "                    print(f\"   Topics: {', '.join(course['topics'])}\")\n",
    "                print()\n",
    "            ndcg = evaluate(query, results[:limit])\n",
    "            print(f\"NDCG@10: {ndcg:.3f}\")\n",
    "\n",
    "submit_button.on_click(on_submit)\n",
    "query_input.on_submit(lambda widget: on_submit(None))\n",
    "display(query_input, index_selector, max_results, submit_button, output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
