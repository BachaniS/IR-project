{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully parsed 172 courses.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/someshb/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "/var/folders/sz/3_3bz7f50bs3023_2hj47k5h0000gn/T/ipykernel_97179/2713317633.py:414: DeprecationWarning: on_submit is deprecated. Instead, set the .continuous_update attribute to False and observe the value changing with: mywidget.observe(callback, 'value').\n",
      "  query_input.on_submit(on_enter)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8440d480a3cc4ae9b6e22d7ba52dc6a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Query:', placeholder='Enter your query...')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc01b690e57a48b4909af19afa58427e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "RadioButtons(description='Index:', index=2, options=('Inverted Index', 'BM25', 'Both'), value='Both')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49dc0fd48d844c668e60e022ed485309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=10, description='Max Results:', max=50, min=5, step=5)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9b0041a2251493591deb7df6b843880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Search', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "092378359faf4fc2a7dd8afcb35eaab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import nltk\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def parse_courses(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    courses = []\n",
    "    \n",
    "    # Find all course blocks\n",
    "    for course_block in soup.find_all('div', class_='courseblock'):\n",
    "        # Extract course code and title\n",
    "        title_element = course_block.find('p', class_='courseblocktitle')\n",
    "        if title_element and title_element.find('strong'):\n",
    "            title_text = title_element.find('strong').text.strip()\n",
    "            \n",
    "            # Parse the course code and title\n",
    "            # Format: \"CS 1100. Computer Science and Its Applications. (4 Hours)\"\n",
    "            code_title_pattern = r'([A-Z]+\\s*\\d+)\\.\\s*(.*?)\\.\\s*\\(.*?\\)'\n",
    "            match = re.match(code_title_pattern, title_text)\n",
    "            \n",
    "            if match:\n",
    "                code = match.group(1).replace(' ', '')  # Remove spaces in code\n",
    "                title = match.group(2).strip()\n",
    "                \n",
    "                # Extract description\n",
    "                desc_element = course_block.find('p', class_='cb_desc')\n",
    "                desc = desc_element.text.strip() if desc_element else \"\"\n",
    "                \n",
    "                # Extract prerequisites and corequisites if available\n",
    "                prereqs = []\n",
    "                coreqs = []\n",
    "                for extra in course_block.find_all('p', class_='courseblockextra'):\n",
    "                    if 'Prerequisite' in extra.text:\n",
    "                        prereq_links = extra.find_all('a', class_='bubblelink')\n",
    "                        prereqs = [link.text.strip() for link in prereq_links]\n",
    "                    elif 'Corequisite' in extra.text:\n",
    "                        coreq_links = extra.find_all('a', class_='bubblelink')\n",
    "                        coreqs = [link.text.strip() for link in coreq_links]\n",
    "                \n",
    "                # Extract course level\n",
    "                level_match = re.search(r'(\\d{4})', code)\n",
    "                level = int(level_match.group(1)) if level_match else 0\n",
    "                \n",
    "                courses.append({\n",
    "                    'code': code,\n",
    "                    'title': title,\n",
    "                    'description': desc,\n",
    "                    'prerequisites': prereqs,\n",
    "                    'corequisites': coreqs,\n",
    "                    'level': level\n",
    "                })\n",
    "    \n",
    "    return courses\n",
    "\n",
    "# Download NLTK stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Expanded synonyms dictionary\n",
    "synonyms = {\n",
    "    'ai': ['artificial', 'intelligence', 'ai'],\n",
    "    'ml': ['machine', 'learning', 'ml'],\n",
    "    'programming': ['coding', 'development', 'programming'],\n",
    "    'web': ['website', 'internet', 'web'],\n",
    "    'data': ['information', 'records', 'data'],\n",
    "    'vr': ['virtual', 'reality', 'vr', 'virtualreality'],\n",
    "    'ar': ['augmented', 'reality', 'ar'],\n",
    "    'cs': ['computer', 'science', 'cs'],\n",
    "    'it': ['information', 'technology', 'it'],\n",
    "    'cybersecurity': ['cyber', 'security', 'cybersecurity'],\n",
    "    'database': ['db', 'data', 'base'],\n",
    "    'network': ['net', 'work', 'network'],\n",
    "    'software': ['app', 'application', 'software'],\n",
    "    'ir': ['information', 'retrieval', 'ir'],\n",
    "    'hci': ['human', 'computer', 'interaction', 'hci'],\n",
    "    'graphics': ['graphic', 'design', 'graphics'],\n",
    "    'algorithms': ['algorithm', 'algorithms'],\n",
    "    'theory': ['theoretical', 'concepts', 'theory'],\n",
    "    'systems': ['system', 'infrastructure', 'systems'],\n",
    "    'nlp': ['natural', 'language', 'processing', 'nlp']\n",
    "}\n",
    "\n",
    "def process_query(query):\n",
    "    # Tokenization and Lowercasing\n",
    "    tokens = re.findall(r'\\b\\w+\\b', query.lower())\n",
    "    # Stopword Removal\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    # Stemming\n",
    "    stemmed_tokens = [stemmer.stem(t) for t in tokens]\n",
    "    \n",
    "    # Combine original and stemmed tokens\n",
    "    all_tokens = list(set(tokens + stemmed_tokens))\n",
    "    \n",
    "    # Synonym Expansion\n",
    "    expanded = []\n",
    "    for t in all_tokens:\n",
    "        if t in synonyms:\n",
    "            expanded.extend(synonyms[t])\n",
    "        else:\n",
    "            expanded.append(t)\n",
    "    \n",
    "    return list(set(expanded))  # Remove duplicates\n",
    "\n",
    "def build_inverted_index(courses):\n",
    "    index = {}\n",
    "    for course in courses:\n",
    "        text = f\"{course['code']} {course['title']} {course['description']}\".lower()\n",
    "        tokens = process_query(text)\n",
    "        for token in set(tokens):  # Deduplicate per document\n",
    "            if token not in index:\n",
    "                index[token] = []\n",
    "            index[token].append(course['code'])\n",
    "    return index\n",
    "\n",
    "def build_bm25_index(courses):\n",
    "    bm25_index = defaultdict(dict)\n",
    "    doc_lengths = {}\n",
    "    df = defaultdict(int)\n",
    "    N = len(courses)\n",
    "    \n",
    "    # Precompute term frequencies and document frequencies\n",
    "    for course in courses:\n",
    "        text = f\"{course['code']} {course['title']} {course['description']}\".lower()\n",
    "        tokens = process_query(text)\n",
    "        doc_lengths[course['code']] = len(tokens)\n",
    "        tf = defaultdict(int)\n",
    "        for token in tokens:\n",
    "            tf[token] += 1\n",
    "        for token in set(tokens):\n",
    "            df[token] += 1\n",
    "        bm25_index[course['code']] = dict(tf)  # Convert to regular dict\n",
    "    \n",
    "    # Store additional stats\n",
    "    avg_dl = sum(doc_lengths.values()) / N if N > 0 else 0\n",
    "    return bm25_index, df, doc_lengths, avg_dl\n",
    "\n",
    "def bm25_score(query_terms, doc_id, bm25_index, df, doc_lengths, avg_dl, k1=1.5, b=0.75):\n",
    "    score = 0.0\n",
    "    N = len(doc_lengths)\n",
    "    \n",
    "    # Check if doc_id exists in the index\n",
    "    if doc_id not in bm25_index:\n",
    "        return 0.0\n",
    "        \n",
    "    tf = bm25_index[doc_id]\n",
    "    dl = doc_lengths.get(doc_id, 0)\n",
    "    \n",
    "    for term in query_terms:\n",
    "        if term in tf:\n",
    "            # Fixed IDF calculation to avoid negative values\n",
    "            idf = max(0, math.log((N - df[term] + 0.5) / (df[term] + 0.5) + 1))\n",
    "            term_score = idf * (tf[term] * (k1 + 1)) / (tf[term] + k1 * (1 - b + b * dl / avg_dl)) if avg_dl > 0 else 0\n",
    "            score += term_score\n",
    "    \n",
    "    return score\n",
    "\n",
    "def pagerank_score(course, query_terms):\n",
    "    # Base score from course level\n",
    "    level_score = min(1.0, course['level'] / 6000) if course['level'] > 0 else 0  # Normalize to 0-1 range\n",
    "    \n",
    "    # Title match bonus (higher rank if query terms appear in title)\n",
    "    title_words = set(process_query(course['title']))\n",
    "    title_match = len(set(query_terms).intersection(title_words)) / max(1, len(query_terms))\n",
    "    \n",
    "    # Description match bonus\n",
    "    desc_words = set(process_query(course['description']))\n",
    "    desc_match = len(set(query_terms).intersection(desc_words)) / max(1, len(query_terms))\n",
    "    \n",
    "    # Combine scores with weights\n",
    "    return (level_score * 0.3) + (title_match * 0.5) + (desc_match * 0.2)\n",
    "\n",
    "def apply_filters(results, query):\n",
    "    # Process query for negation detection\n",
    "    query_lower = query.lower()\n",
    "    \n",
    "    # Check for negation patterns\n",
    "    not_masters = any(pattern in query_lower for pattern in [\"not master\", \"not graduate\", \"no master\", \"no graduate\"])\n",
    "    not_undergrad = any(pattern in query_lower for pattern in [\"not undergrad\", \"not undergraduate\", \"no undergrad\", \"no undergraduate\"])\n",
    "    \n",
    "    # Level-based filters\n",
    "    level_filters = {}\n",
    "    \n",
    "    # Add standard level filters\n",
    "    if \"master\" in query_lower or \"masters\" in query_lower or \"master's\" in query_lower or \"graduate\" in query_lower:\n",
    "        # Only if not negated\n",
    "        if not not_masters:\n",
    "            level_filters[\"masters\"] = lambda c: c['level'] >= 5000\n",
    "    \n",
    "    if \"undergrad\" in query_lower or \"undergraduate\" in query_lower:\n",
    "        # Only if not negated\n",
    "        if not not_undergrad:\n",
    "            level_filters[\"undergraduate\"] = lambda c: c['level'] < 5000\n",
    "    \n",
    "    # Add negation filters\n",
    "    if not_masters:\n",
    "        level_filters[\"not_masters\"] = lambda c: c['level'] < 5000\n",
    "    \n",
    "    if not_undergrad:\n",
    "        level_filters[\"not_undergrad\"] = lambda c: c['level'] >= 5000\n",
    "    \n",
    "    # Name/content-based filters\n",
    "    name_filters = {\n",
    "        \"advanced\": lambda c: \"advanced\" in c['title'].lower() or \"advanced\" in c['description'].lower(),\n",
    "        \"intro\": lambda c: any(word in c['title'].lower() for word in [\"intro\", \"introduction\", \"introductory\"]) or \n",
    "                         any(word in c['description'].lower() for word in [\"intro\", \"introduction\", \"introductory\"]),\n",
    "        \"introductory\": lambda c: any(word in c['title'].lower() for word in [\"intro\", \"introduction\", \"introductory\"]) or \n",
    "                               any(word in c['description'].lower() for word in [\"intro\", \"introduction\", \"introductory\"]),\n",
    "        \"beginner\": lambda c: any(word in c['title'].lower() for word in [\"beginner\", \"beginning\", \"elementary\", \"fundamental\"]) or\n",
    "                           any(word in c['description'].lower() for word in [\"beginner\", \"beginning\", \"elementary\", \"fundamental\"])\n",
    "    }\n",
    "    \n",
    "    # Combine all filters\n",
    "    all_filters = {**level_filters, **name_filters}\n",
    "    \n",
    "    # Extract active filters and build a clean query\n",
    "    active_filters = []\n",
    "    filter_terms = set(name_filters.keys()) | {\"master\", \"masters\", \"master's\", \"graduate\", \"undergrad\", \"undergraduate\", \n",
    "                                              \"not\", \"no\"}\n",
    "    \n",
    "    # Clean query by removing filter terms and negation patterns\n",
    "    clean_query = query_lower\n",
    "    for term in filter_terms:\n",
    "        clean_query = re.sub(r'\\b' + re.escape(term) + r'\\b', '', clean_query)\n",
    "    \n",
    "    # Remove negation patterns\n",
    "    clean_query = re.sub(r'not\\s+\\w+', '', clean_query)\n",
    "    clean_query = re.sub(r'no\\s+\\w+', '', clean_query)\n",
    "    \n",
    "    # Apply all level filters\n",
    "    for filter_func in level_filters.values():\n",
    "        active_filters.append(filter_func)\n",
    "    \n",
    "    # Apply name filters only if they appear in the query\n",
    "    for term, filter_func in name_filters.items():\n",
    "        if term in query_lower:\n",
    "            active_filters.append(filter_func)\n",
    "    \n",
    "    # Clean up the query\n",
    "    clean_query = re.sub(r'\\s+', ' ', clean_query).strip()\n",
    "    \n",
    "    # If no filters specified, return all results\n",
    "    if not active_filters:\n",
    "        return results, clean_query\n",
    "    \n",
    "    # Apply all active filters\n",
    "    filtered_results = []\n",
    "    for course in results:\n",
    "        if all(f(course) for f in active_filters):\n",
    "            filtered_results.append(course)\n",
    "    \n",
    "    return filtered_results, clean_query\n",
    "\n",
    "def search_courses(query, index_method='Both'):\n",
    "    query = query.strip()\n",
    "    if not query:\n",
    "        return []\n",
    "    \n",
    "    # First apply filters (and get clean query without filter terms)\n",
    "    results = courses  # Start with all courses\n",
    "    results, clean_query = apply_filters(results, query)\n",
    "    \n",
    "    # If clean_query is empty (only filter terms), return filtered results\n",
    "    if not clean_query.strip():\n",
    "        return results\n",
    "    \n",
    "    # Continue with normal search on cleaned query\n",
    "    processed_terms = process_query(clean_query)\n",
    "    if not processed_terms:\n",
    "        return results  # Return filtered results if no search terms left\n",
    "    \n",
    "    combined_scores = defaultdict(float)\n",
    "    \n",
    "    # Inverted index lookup\n",
    "    if index_method in ['Inverted Index', 'Both']:\n",
    "        doc_codes = set()\n",
    "        for term in processed_terms:\n",
    "            if term in inverted_index:\n",
    "                doc_codes.update(inverted_index[term])\n",
    "        \n",
    "        # Only consider courses that passed the filters\n",
    "        filtered_codes = {course['code'] for course in results}\n",
    "        doc_codes = doc_codes.intersection(filtered_codes)\n",
    "        \n",
    "        for code in doc_codes:\n",
    "            combined_scores[code] += 1.0\n",
    "    \n",
    "    # BM25 scoring - only for courses that passed the filters\n",
    "    if index_method in ['BM25', 'Both']:\n",
    "        for course in results:\n",
    "            code = course['code']\n",
    "            score = bm25_score(processed_terms, code, bm25_index, df, doc_lengths, avg_dl)\n",
    "            if score > 0:\n",
    "                combined_scores[code] += min(5.0, score)\n",
    "    \n",
    "    # If we have scores, return ranked results\n",
    "    if combined_scores:\n",
    "        # Get unique course objects\n",
    "        course_dict = {c['code']: c for c in results}\n",
    "        scored_courses = []\n",
    "        \n",
    "        for code, score in combined_scores.items():\n",
    "            if code in course_dict:\n",
    "                course = course_dict[code]\n",
    "                # Add pagerank component to score\n",
    "                pr_score = pagerank_score(course, processed_terms)\n",
    "                final_score = score + (pr_score * 2.0)  # Weighted combination\n",
    "                scored_courses.append((course, final_score))\n",
    "        \n",
    "        # Sort by combined score\n",
    "        scored_courses.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [s[0] for s in scored_courses]\n",
    "    \n",
    "    # If no scores (no matching terms), return filtered results\n",
    "    return results\n",
    "\n",
    "# Load HTML content\n",
    "try:\n",
    "    with open('cs_catalog.html', 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "    \n",
    "    courses = parse_courses(html_content)\n",
    "    print(f\"Successfully parsed {len(courses)} courses.\")\n",
    "    \n",
    "    # Create indices\n",
    "    inverted_index = build_inverted_index(courses)\n",
    "    bm25_index, df, doc_lengths, avg_dl = build_bm25_index(courses)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading or parsing course data: {e}\")\n",
    "    courses = []\n",
    "    inverted_index = {}\n",
    "    bm25_index, df, doc_lengths, avg_dl = {}, {}, {}, 0\n",
    "\n",
    "# Query input\n",
    "query_input = widgets.Text(\n",
    "    placeholder='Enter your query...',\n",
    "    description='Query:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Index selection\n",
    "index_selector = widgets.RadioButtons(\n",
    "    options=['Inverted Index', 'BM25', 'Both'],\n",
    "    description='Index:',\n",
    "    disabled=False,\n",
    "    value='Both'  # Default selection\n",
    ")\n",
    "\n",
    "# Maximum results selector\n",
    "max_results = widgets.IntSlider(\n",
    "    value=10,\n",
    "    min=5,\n",
    "    max=50,\n",
    "    step=5,\n",
    "    description='Max Results:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Submit button\n",
    "submit_button = widgets.Button(description='Search')\n",
    "\n",
    "# Output display\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_submit(b):\n",
    "    query = query_input.value\n",
    "    \n",
    "    # Search for courses\n",
    "    results = search_courses(query, index_selector.value)\n",
    "    \n",
    "    # Display results\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        print(f\"Results for query: '{query}'\")\n",
    "        \n",
    "        if not results:\n",
    "            print(\"No matching courses found.\")\n",
    "        else:\n",
    "            limit = min(len(results), max_results.value)\n",
    "            for i, course in enumerate(results[:limit]):\n",
    "                print(f\"{i+1}. {course['code']}: {course['title']}\")\n",
    "                \n",
    "                # Display prerequisites if available\n",
    "                if course['prerequisites']:\n",
    "                    print(f\"   Prerequisites: {', '.join(course['prerequisites'])}\")\n",
    "                    \n",
    "                # Display corequisites if available\n",
    "                if course['corequisites']:\n",
    "                    print(f\"   Corequisites: {', '.join(course['corequisites'])}\")\n",
    "                    \n",
    "                print(f\"   Level: {course['level']}\")\n",
    "                \n",
    "                # Display description with truncation if too long\n",
    "                desc = course['description']\n",
    "                if len(desc) > 100:\n",
    "                    print(f\"   {desc[:100]}...\")\n",
    "                else:\n",
    "                    print(f\"   {desc}\")\n",
    "                print()\n",
    "\n",
    "# Event handler for submit button\n",
    "submit_button.on_click(on_submit)\n",
    "\n",
    "# Event handler for Enter key in query input\n",
    "def on_enter(widget):\n",
    "    on_submit(None)\n",
    "query_input.on_submit(on_enter)\n",
    "\n",
    "# Display widgets\n",
    "display(query_input, index_selector, max_results, submit_button, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Course data exported to courses.xml\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import xml.dom.minidom as minidom\n",
    "\n",
    "def export_to_xml(courses, filename=\"courses.xml\"):\n",
    "    \"\"\"\n",
    "    Export the parsed course data to an XML file.\n",
    "    \n",
    "    Parameters:\n",
    "    courses (list): List of course dictionaries\n",
    "    filename (str): Name of the output XML file\n",
    "    \"\"\"\n",
    "    # Create the root element\n",
    "    root = ET.Element(\"courses\")\n",
    "    \n",
    "    # Add each course as a child element\n",
    "    for course in courses:\n",
    "        course_elem = ET.SubElement(root, \"course\")\n",
    "        \n",
    "        # Add course code\n",
    "        code_elem = ET.SubElement(course_elem, \"code\")\n",
    "        code_elem.text = course['code']\n",
    "        \n",
    "        # Add course title\n",
    "        title_elem = ET.SubElement(course_elem, \"title\")\n",
    "        title_elem.text = course['title']\n",
    "        \n",
    "        # Add course description\n",
    "        description_elem = ET.SubElement(course_elem, \"description\")\n",
    "        description_elem.text = course['description']\n",
    "        \n",
    "        # Add course level\n",
    "        level_elem = ET.SubElement(course_elem, \"level\")\n",
    "        level_elem.text = str(course['level'])\n",
    "        \n",
    "        # Add prerequisites if available\n",
    "        if course['prerequisites']:\n",
    "            prereqs_elem = ET.SubElement(course_elem, \"prerequisites\")\n",
    "            for prereq in course['prerequisites']:\n",
    "                prereq_elem = ET.SubElement(prereqs_elem, \"prerequisite\")\n",
    "                prereq_elem.text = prereq\n",
    "        \n",
    "        # Add corequisites if available\n",
    "        if course['corequisites']:\n",
    "            coreqs_elem = ET.SubElement(course_elem, \"corequisites\")\n",
    "            for coreq in course['corequisites']:\n",
    "                coreq_elem = ET.SubElement(coreqs_elem, \"corequisite\")\n",
    "                coreq_elem.text = coreq\n",
    "    \n",
    "    # Convert to string and pretty print\n",
    "    rough_string = ET.tostring(root, 'utf-8')\n",
    "    reparsed = minidom.parseString(rough_string)\n",
    "    pretty_xml = reparsed.toprettyxml(indent=\"  \")\n",
    "    \n",
    "    # Write to file\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(pretty_xml)\n",
    "    \n",
    "    return filename\n",
    "\n",
    "# Example usage:\n",
    "# After parsing the courses from HTML\n",
    "filename = export_to_xml(courses)\n",
    "print(f\"Course data exported to {filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
